{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "current_guitarLSTM2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF2uyPfxgi8H"
      },
      "source": [
        "# TO USE: \n",
        "#    1. Upload your recorded tone sample .wav file from the SmartAmpPro plugin.\n",
        "#        Note: Optionally, upload two separate .wav files and set flags for two inputs\n",
        "#        Note: Currently supports 32 Bit FLoating Point Wav, or 16 Bit PCM;\n",
        "#                Using 16 Bit PCM (CD quality) allows for smaller .wav files\n",
        "#    2. Select a runtime (GPU recommended)\n",
        "#    3. Change the \"in_file\" variable to match your uploaded file. \n",
        "#        Note: If two .wav files used, set \"out_file\" to match uploaded file\n",
        "#    4 Change the \"name\" file to the desired model name\n",
        "#    5. Click \"Runtime\" then \"Run All\"\n",
        "#\n",
        "#     Note: Tested on CPU and GPU runtimes.\n",
        "#     Note: Uses MSE for loss calculation instead of Error to Signal with Pre-emphasis filter\n",
        "\n",
        "# EDIT THIS SECTION FOR USER INPUTS ###########################################\n",
        "#\n",
        "name = 'my_model'\n",
        "in_file = 'in.wav'\n",
        "\n",
        "# If using two mono .wav files, enter uploaded out_file here\n",
        "# If using a single stereo .wav file recorded from SmartAmpPro,\n",
        "#    ensure out_file=''\n",
        "out_file = 'out.wav'\n",
        "\n",
        "## Training Parameters\n",
        "epochs = 30  # Increase epochs to try extended training to improve model results \n",
        "learning_rate = 0.001  # Agressiveness of training\n",
        "hidden_units = 24     # Number of units in LSTM layer to train\n",
        "\n",
        "input_size = 120 # Using input_size greater than 140 will not work in SmartAmpPro\n",
        "\n",
        "batch_size = 4096 # Number of samples to train at a time\n",
        "test_size = 0.2 # Portion of input data to use in testing (.2 = 20%)\n",
        "###############################################################################\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Conv1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.activations import tanh, elu, relu\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "import os\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import h5py"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U22mDBe4jaf2"
      },
      "source": [
        "if not os.path.exists('models/'+name):\n",
        "    os.makedirs('models/'+name)\n",
        "else:\n",
        "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
        "    exit"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dOqZHalR5zf"
      },
      "source": [
        "class WindowArray(Sequence):\n",
        "        \n",
        "    def __init__(self, x, y, window_len, batch_size=32):\n",
        "        self.x = x\n",
        "        self.y = y[window_len-1:] \n",
        "        self.window_len = window_len\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return (len(self.x) - self.window_len +1) // self.batch_size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x_out = np.stack([self.x[idx: idx+self.window_len] for idx in range(index*self.batch_size, (index+1)*self.batch_size)])\n",
        "        y_out = self.y[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        return x_out, y_out"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqI-cGt1jaG2"
      },
      "source": [
        "def save_wav(name, data):\n",
        "    wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
        "\n",
        "def normalize(data):\n",
        "    data_max = max(data)\n",
        "    data_min = min(data)\n",
        "    data_norm = max(data_max,abs(data_min))\n",
        "    return data / data_norm\n",
        "\n",
        "\n",
        "'''This is a similar Tensorflow/Keras implementation of the LSTM model from the paper:\n",
        "    \"Real-Time Guitar Amplifier Emulation with Deep Learning\"\n",
        "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
        "\n",
        "    Uses a stack of two 1-D Convolutional layers, followed by LSTM, followed by \n",
        "    a Dense (fully connected) layer. Three preset training modes are available, \n",
        "    with further customization by editing the code. A Sequential tf.keras model \n",
        "    is implemented here.\n",
        "'''\n",
        "# Preset parameters specifically for running in SmartAmpPro\n",
        "conv1d_strides = 12\n",
        "conv1d_1_strides = 12   \n",
        "conv1d_filters = 4\n",
        "\n",
        "conv1d_KS = conv1d_strides \n",
        "conv1d_1_KS = conv1d_1_strides\n",
        "\n",
        "# Create Sequential Model ###########################################\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv1D(conv1d_filters, 12,strides=conv1d_strides, activation=None, padding='same',input_shape=(input_size,1)))\n",
        "model.add(Conv1D(conv1d_filters, 12,strides=conv1d_strides, activation=None, padding='same'))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation=None))\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mse'])\n",
        "model.summary()\n",
        "\n",
        "# Load and Preprocess Data ###########################################\n",
        "in_rate, stereo_data = wavfile.read(in_file)\n",
        "\n",
        "# If a second wav file is provided, assume each file is mono and load data\n",
        "if out_file != \"\":\n",
        "    out_rate, out_data = wavfile.read(out_file)\n",
        "    in_data = stereo_data\n",
        "# Else, use the stereo wav file channel 1 and channel 2 for training data\n",
        "else:\n",
        "    in_data = stereo_data.T[0]\n",
        "    out_data = stereo_data.T[1]\n",
        "\n",
        "X_all = in_data.astype(np.float32).flatten()  \n",
        "X_all = normalize(X_all).reshape(len(X_all),1)   \n",
        "y_all = out_data.astype(np.float32).flatten() \n",
        "y_all = normalize(y_all).reshape(len(y_all),1)\n",
        "\n",
        "train_examples = int(len(X_all)*0.8)\n",
        "train_arr = WindowArray(X_all[:train_examples], y_all[:train_examples], input_size, batch_size=batch_size)\n",
        "val_arr = WindowArray(X_all[train_examples:], y_all[train_examples:], input_size, batch_size=batch_size)\n",
        "\n",
        "# Train Model ###################################################\n",
        "history = model.fit(train_arr, validation_data=val_arr, epochs=epochs, shuffle=True)    \n",
        "model.save('models/'+name+'/'+name+'.h5')\n",
        "\n",
        "# Run Prediction #################################################\n",
        "print(\"Running prediction..\")\n",
        "\n",
        "# Get the last 20% of the wav data to run prediction and plot results\n",
        "y_the_rest, y_last_part = np.split(y_all, [int(len(y_all)*.8)])\n",
        "x_the_rest, x_last_part = np.split(X_all, [int(len(X_all)*.8)])\n",
        "y_test = y_last_part[input_size-1:] \n",
        "test_arr = WindowArray(x_last_part, y_last_part, input_size, batch_size = batch_size)\n",
        "\n",
        "prediction = model.predict(test_arr)\n",
        "\n",
        "save_wav('models/'+name+'/y_pred.wav', prediction)\n",
        "save_wav('models/'+name+'/x_test.wav', x_last_part)\n",
        "save_wav('models/'+name+'/y_test.wav', y_test)\n",
        "\n",
        "# Add additional data to the saved model (like input_size)\n",
        "filename = 'models/'+name+'/'+name+'.h5'\n",
        "f = h5py.File(filename, 'a')\n",
        "grp = f.create_group(\"info\")\n",
        "dset = grp.create_dataset(\"input_size\", (1,), dtype='int16')\n",
        "dset[0] = input_size\n",
        "dset2 = grp.create_dataset(\"conv1d_stride\", (1,), dtype='int16')\n",
        "dset3 = grp.create_dataset(\"conv1d_1_stride\", (1,), dtype='int16')\n",
        "dset[0] = input_size\n",
        "dset2[0] = conv1d_strides\n",
        "dset3[0] = conv1d_1_strides\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WygMJjMlSsV0"
      },
      "source": [
        "# Generate json model ################################\r\n",
        "import json\r\n",
        "filename = 'models/'+name+'/'+ name +'.h5'\r\n",
        "json_filename = 'models/'+name+'/'+ name\r\n",
        "f = h5py.File(filename, 'r')\r\n",
        "\r\n",
        "# Load the model data\r\n",
        "data = {}\r\n",
        "for layer in f[\"model_weights\"].keys():\r\n",
        "    if layer not in data.keys():\r\n",
        "        data[layer] = {}\r\n",
        "    for item in f[\"model_weights\"][layer][layer]:\r\n",
        "        if item not in data[layer].keys():\r\n",
        "            data[layer][item] = {}\r\n",
        "            try:\r\n",
        "                data[layer][item] = f[\"model_weights\"][layer][layer][item][:].tolist()\r\n",
        "            except:\r\n",
        "                data[layer][\"kernel:0\"] = f[\"model_weights\"][layer][layer][item][\"kernel:0\"][:].tolist()\r\n",
        "                data[layer][\"bias:0\"] = f[\"model_weights\"][layer][layer][item][\"bias:0\"][:].tolist()\r\n",
        "        else:\r\n",
        "            try:\r\n",
        "                data[layer][item] = f[\"model_weights\"][layer][layer][item][:].tolist()\r\n",
        "            except:\r\n",
        "                data[layer][\"kernel:0\"] = f[\"model_weights\"][layer][layer][item][\"kernel:0\"][:].tolist()\r\n",
        "                data[layer][\"bias:0\"] = f[\"model_weights\"][layer][layer][item][\"bias:0\"][:].tolist()\r\n",
        "\r\n",
        "input_size = f[\"info\"][\"input_size\"][0]\r\n",
        "conv1d_strides = f[\"info\"][\"conv1d_stride\"][0]\r\n",
        "conv1d_1_strides = f[\"info\"][\"conv1d_1_stride\"][0]\r\n",
        "data['input_size'] = int(input_size)\r\n",
        "print(\"input_size: \",input_size)\r\n",
        "data['conv1d_stride'] = int(conv1d_strides)\r\n",
        "data['conv1d_1_stride'] = int(conv1d_1_strides)\r\n",
        "f.close()\r\n",
        "\r\n",
        "with open(json_filename + \".json\", 'w') as outfile:\r\n",
        "    json.dump(data, outfile)\r\n",
        "print(\"SmartAmpPro model generated: \", json_filename + \".json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf-zAc_cS5Uu"
      },
      "source": [
        "# Model Evaluation and Plotting\r\n",
        "\r\n",
        "def read_wave(wav_file):\r\n",
        "    # Extract Audio and framerate from Wav File\r\n",
        "    fs, signal = wavfile.read(wav_file)\r\n",
        "    return signal, fs\r\n",
        "\r\n",
        "\r\n",
        "def analyze_pred_vs_actual(output_wav, pred_wav, input_wav, model_name, show_plots, path):\r\n",
        "    \"\"\"Generate plots to analyze the predicted signal vs the actual\r\n",
        "    signal.\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "        output_wav : The actual signal, by default will use y_test.wav from the test.py output\r\n",
        "        pred_wav : The predicted signal, by default will use y_pred.wav from the test.py output\r\n",
        "        input_wav : The pre effect signal, by default will use x_test.wav from the test.py output\r\n",
        "        model_name : Used to add the model name to the plot .png filename\r\n",
        "        path   :   The save path for generated .png figures\r\n",
        "        show_plots : Default is 1 to show plots, 0 to only generate .png files and suppress plots\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Read the input wav file\r\n",
        "    signal3, fs3 = read_wave(input_wav)\r\n",
        "\r\n",
        "    # Read the output wav file\r\n",
        "    signal1, fs = read_wave(output_wav)\r\n",
        "\r\n",
        "    Time = np.linspace(0, len(signal1) / fs, num=len(signal1))\r\n",
        "    fig, (ax1, ax3) = plt.subplots(2, sharex=True, figsize=(13, 8))\r\n",
        "    fig.suptitle(\"Predicted vs Actual Signal\")\r\n",
        "    ax1.plot(Time, signal1, label=output_wav, color=\"red\")\r\n",
        "\r\n",
        "    # Read the predicted wav file\r\n",
        "    signal2, fs2 = read_wave(pred_wav)\r\n",
        "\r\n",
        "    Time2 = np.linspace(0, len(signal2) / fs2, num=len(signal2))\r\n",
        "    ax1.plot(Time2, signal2, label=pred_wav, color=\"green\")\r\n",
        "    ax1.legend(loc=\"upper right\")\r\n",
        "    ax1.set_xlabel(\"Time (s)\")\r\n",
        "    ax1.set_ylabel(\"Amplitude\")\r\n",
        "    ax1.set_title(\"Wav File Comparison\")\r\n",
        "    ax1.grid(\"on\")\r\n",
        "\r\n",
        "    error_list = []\r\n",
        "    for s1, s2 in zip(signal1, signal2):\r\n",
        "        error_list.append(abs(s2 - s1))\r\n",
        "\r\n",
        "\r\n",
        "    # Plot the original signal\r\n",
        "    Time3 = np.linspace(0, len(signal3) / fs3, num=len(signal3))\r\n",
        "    ax3.plot(Time3, signal3, label=input_wav, color=\"purple\")\r\n",
        "    ax3.legend(loc=\"upper right\")\r\n",
        "    ax3.set_xlabel(\"Time (s)\")\r\n",
        "    ax3.set_ylabel(\"Amplitude\")\r\n",
        "    ax3.set_title(\"Original Input\")\r\n",
        "    ax3.grid(\"on\")\r\n",
        "\r\n",
        "    # Save the plot\r\n",
        "    plt.savefig(path+'/'+model_name + \"_signal_comparison.png\", bbox_inches=\"tight\")\r\n",
        "    # Create a zoomed in plot of 0.01 seconds centered at the max input signal value\r\n",
        "    sig_temp = signal1.tolist()\r\n",
        "    plt.axis(\r\n",
        "        [\r\n",
        "            Time3[sig_temp.index((max(sig_temp)))] - 0.005,\r\n",
        "            Time3[sig_temp.index((max(sig_temp)))] + 0.005,\r\n",
        "            min(signal2),\r\n",
        "            max(signal2),\r\n",
        "        ]\r\n",
        "    )\r\n",
        "   \r\n",
        "    plt.savefig(path+'/'+model_name + \"_Detail_signal_comparison.png\", bbox_inches=\"tight\")\r\n",
        "    plt.show() # Uncomment this line to show Detail plot\r\n",
        "    # Reset the axis\r\n",
        "    plt.axis([0, Time3[-1], min(signal2), max(signal2)])\r\n",
        "\r\n",
        "\r\n",
        "print(\"Plotting results..\")\r\n",
        "\r\n",
        "\r\n",
        "analyze_pred_vs_actual('models/'+name+'/y_test.wav',\r\n",
        "                       'models/'+name+'/y_pred.wav', \r\n",
        "                       'models/'+name+'/x_test.wav',\r\n",
        "                       name,\r\n",
        "                       1,\r\n",
        "                       'models/'+name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}